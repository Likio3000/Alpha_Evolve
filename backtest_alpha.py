import argparse
import re
import glob
import os
import math
import textwrap
import sys
from pathlib import Path
from typing import List, Dict

import numpy as np
import pandas as pd

"""backtest_alpha.py — v0.2
================================
Back‑test one **gene‑expression alpha** (or a list of them) on a folder full of 4‑hour OHLC CSVs.

Upgrades vs v0.1
----------------
1. **Signal scaling** – `--scale {zscore,rank,sign}` (defaults to *zscore* clipped ±1).
2. **Holding period** – `--hold N` (bars) → position is the *average* of the last N signals (N=1 ⇢ one‑bar hold).
3. **|gross| cap** – |pos| ≤ 1 after scaling/holding.
4. **Turnover + fees** – turnover = mean |Δpos|, commission = fee × |Δpos|.
"""

###############################################################################################
# ░░ CLI ░░######################################################################################
###############################################################################################


def _parse_cli() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        description="Back‑test alpha expression(s) on OHLC data"
    )
    p.add_argument(
        "expr",
        help="Expression string ­or­ a .txt file containing one expression per line",
    )
    p.add_argument(
        "-d", "--data_dir", default="./data", help="Directory with *.csv OHLC data"
    )
    p.add_argument(
        "-f",
        "--fee",
        type=float,
        default=0.0,
        help="Round‑trip fee in basis‑points (1 bp = 0.0001)",
    )
    p.add_argument(
        "--scale",
        choices=["zscore", "rank", "sign"],
        default="zscore",
        help="How to transform raw signal into a position (default: zscore, clipped ±1)",
    )
    p.add_argument(
        "--hold",
        type=int,
        default=1,
        help="Holding period in bars (default 1 = one‑bar hold)",
    )
    p.add_argument(
        "--lag",
        type=int,
        default=1,
        help="Bars to delay entering the position (default 1)",
    )
    p.add_argument("-s", "--start", help="Start date (YYYY‑MM‑DD)")
    p.add_argument("-e", "--end", help="End date (YYYY‑MM‑DD)")
    p.add_argument(
        "-o", "--out", default="alpha_backtest_results.csv", help="Output csv path"
    )
    p.add_argument(
        "--summary", action="store_true", help="Print equal‑weighted portfolio stats"
    )
    return p.parse_args()


###############################################################################################
# ░░ Expression parser ░░ #####################################################################
###############################################################################################

_UNARY = {
    "tanh": np.tanh,
    "sign": np.sign,
    "neg": np.negative,
    "abs": np.abs,
}

_BINARY = {
    "add": np.add,
    "sub": np.subtract,
    "mul": np.multiply,
    "div": lambda a, b: np.divide(a, np.where(b == 0, 1e-12, b)),
}

_TOKEN = r"[a-zA-Z_][a-zA-Z_0-9]*|\d+(?:\.\d+)?|[()]"


class _Node:
    def __init__(self, label: str, children: List["_Node"] | None = None):
        self.label, self.children = label, children or []

    # ░ evaluation ░
    def eval(self, df: pd.DataFrame) -> np.ndarray:
        if self.label in _UNARY:
            return _UNARY[self.label](self.children[0].eval(df))
        if self.label in _BINARY:
            return _BINARY[self.label](
                self.children[0].eval(df), self.children[1].eval(df)
            )
        # column or numeric literal
        if self.label in df.columns:
            return df[self.label].values
        try:
            return np.full(len(df), float(self.label))
        except ValueError:
            raise KeyError(f"Unknown token: {self.label}")

    # ░ pretty ░
    def __str__(self):
        if not self.children:
            return self.label
        if self.label in _UNARY:
            return f"{self.label}({self.children[0]})"
        return f"({self.children[0]} {self.label} {self.children[1]})"


# ── recursive‑descent ────────────────────────────────────────────────────────── #

# -------------------------------
_TOKEN = r"[A-Za-z_][A-Za-z_0-9]*|\d+(?:\.\d+)?|[()]"


def _parse_expr(expr: str) -> _Node:
    """
    Robust recursive-descent parser for ‘alpha’ grammar:

        expr   := factor { binop factor }*
        factor := terminal
                | unary_name '(' expr ')'
                | '(' expr ')'

    It tolerates super-long formulas generated by the GP without needing
    every binary group wrapped in an extra pair of parentheses.
    """
    tokens = re.findall(_TOKEN, expr)
    pos = 0
    n = len(tokens)

    def next_tok() -> str:  # current token (or '')
        return tokens[pos] if pos < n else ""

    def consume(expected: str | None = None) -> str:
        nonlocal pos
        tok = next_tok()
        if expected and tok != expected:
            raise ValueError(f"expected '{expected}', got '{tok or 'EOF'}'")
        pos += 1
        return tok

    def parse_factor() -> _Node:
        tok = next_tok()

        # unary prefix
        if tok in _UNARY:
            consume()  # unary name
            consume("(")
            child = parse_expr_inner()
            consume(")")
            return _Node(tok, [child])

        # parenthesised sub-expr
        if tok == "(":
            consume("(")
            node = parse_expr_inner()
            consume(")")
            return node

        # terminal
        consume()
        return _Node(tok)

    def parse_expr_inner() -> _Node:
        node = parse_factor()
        while next_tok() in _BINARY:
            op = consume()  # binary operator
            right = parse_factor()
            node = _Node(op, [node, right])
        return node

    root = parse_expr_inner()
    if pos != n:
        raise ValueError(
            f"could not parse complete expression (stopped at token #{pos}/{n})"
        )
    return root


# ---------------------------------------------------------------------------
###############################################################################################
# ░░ Helpers ░░#################################################################################
###############################################################################################


def _rolling_features(df: pd.DataFrame) -> pd.DataFrame:
    for w in (5, 10, 20, 30):
        df[f"ma_{w}"] = df["close"].rolling(w).mean()
        df[f"vol_{w}"] = df["close"].rolling(w).std(ddof=0)
    df["range"] = df["high"] - df["low"]
    df["ret_fwd"] = df["close"].pct_change().shift(-1)
    return df.dropna()


def _max_drawdown(equity: np.ndarray) -> float:
    cum_max = np.maximum.accumulate(equity)
    return (equity / cum_max - 1.0).min()


###############################################################################################
# ░░ Signal ▶ position ░░ ######################################################################
###############################################################################################


def _scale_signal(raw: np.ndarray, method: str) -> np.ndarray:
    if method == "sign":
        return np.sign(raw)
    if method == "rank":
        r = pd.Series(raw).rank(pct=True).values * 2 - 1  # → [-1,1]
        return r
    # default: z‑score, clip ±1
    mu, sd = np.nanmean(raw), np.nanstd(raw)
    z = (raw - mu) / (sd if sd > 0 else 1e-12)
    return np.clip(z, -1, 1)


###############################################################################################
# ░░ Back‑test core ░░ #########################################################################
###############################################################################################


def _backtest_one(
    df: pd.DataFrame,
    sig: np.ndarray,
    *,
    fee_bps: float,
    lag: int,
    hold: int,
    scale: str,
) -> Dict[str, float]:
    # scale raw signal → instantaneous target position in [-1,1]
    inst_pos = _scale_signal(sig, scale)

    # hold‑period smoothing (simple moving‑average of past *hold* signals)
    if hold > 1:
        inst_pos = pd.Series(inst_pos).rolling(hold, min_periods=1).mean().values

    # lag to ensure we act on previous bar info
    pos = pd.Series(inst_pos).shift(lag).fillna(0).values

    # enforce gross cap
    pos = np.clip(pos, -1, 1)

    ret = df["close"].pct_change().fillna(0).values

    dpos = np.diff(pos, prepend=0)
    turnover = np.abs(dpos).mean()

    fee = fee_bps * 1e-4
    strat = pos * ret - fee * np.abs(dpos)

    equity = np.cumprod(1 + strat)
    n = len(strat)
    ann_fac = math.sqrt(6 * 365)

    mu, sigma = strat.mean(), strat.std(ddof=0)
    sharpe = (mu / sigma * ann_fac) if sigma > 0 else 0.0
    ann_ret = equity[-1] ** (365 * 6 / n) - 1.0
    ann_vol = sigma * ann_fac

    return {
        "Sharpe": sharpe,
        "AnnReturn": ann_ret,
        "AnnVol": ann_vol,
        "MaxDD": _max_drawdown(equity),
        "Turnover": turnover,
        "Bars": n,
    }


###############################################################################################
# ░░ Main ░░####################################################################################
###############################################################################################


def main() -> None:
    args = _parse_cli()

    # read expression(s)
    p = Path(args.expr)
    exprs: List[str] = (
        [line.strip() for line in p.read_text().splitlines() if line.strip()]
        if p.exists()
        else [args.expr.strip()]
    )

    # load CSVs
    csvs = sorted(glob.glob(os.path.join(args.data_dir, "*.csv")))
    if not csvs:
        print(f"No CSVs under {args.data_dir}")
        sys.exit(1)

    start = pd.to_datetime(args.start) if args.start else None
    end = pd.to_datetime(args.end) if args.end else None

    all_dfs: Dict[str, pd.DataFrame] = {}
    for f in csvs:
        sym = Path(f).stem
        df = pd.read_csv(f)
        df["time"] = pd.to_datetime(df["time"], unit="s", errors="coerce")
        if start is not None:
            df = df[df["time"] >= start]
        if end is not None:
            df = df[df["time"] <= end]
        all_dfs[sym] = _rolling_features(df.sort_values("time").reset_index(drop=True))
    print(f"Loaded {len(all_dfs)} symbols from {args.data_dir}\n")

    results_rows: List[pd.DataFrame] = []

    for idx, expr in enumerate(exprs, 1):
        node = _parse_expr(expr)
        print("Expression:", textwrap.shorten(expr, 140))
        rows = []
        for sym, df in all_dfs.items():
            sig = node.eval(df)
            metrics = _backtest_one(
                df,
                sig,
                fee_bps=args.fee,
                lag=args.lag,
                hold=max(1, args.hold),
                scale=args.scale,
            )
            rows.append({"Pair": sym, **metrics})
        res = pd.DataFrame(rows).sort_values("Sharpe", ascending=False)
        avg = (
            res[["Sharpe", "AnnReturn", "AnnVol", "MaxDD", "Turnover"]].mean().to_dict()
        )
        print(res.to_string(index=False))
        print(
            "\nEqual‑weight avg ⇒ Sharpe %.3f | AnnRet %.2f%% | AnnVol %.2f%% | MaxDD %.2f%% | Turnover %.3f\n"
            % (
                avg["Sharpe"],
                avg["AnnReturn"] * 100,
                avg["AnnVol"] * 100,
                avg["MaxDD"] * 100,
                avg["Turnover"],
            )
        )
        res.insert(0, "ExprID", idx)
        res.insert(1, "Expr", textwrap.shorten(expr, 60))
        results_rows.append(res)

    full = pd.concat(results_rows, ignore_index=True)
    full.to_csv(args.out, index=False)
    print(f"Results saved ➜ {args.out}\n")


if __name__ == "__main__":
    main()

# TO run the project use the following command:
# # 1) evolve & save programs  (only once per run)
# uv run evolve_alphas.py 3          # or however many generations
#     ↳ produces evolved_top20.pkl

# 2) test them
# uv run backtest_evolved_alphas.py --top 20 --fee 1.0
